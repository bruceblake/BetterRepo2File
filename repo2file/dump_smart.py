import os
import sys
from typing import List, Set, Optional, Dict
import fnmatch
import re
import mimetypes

# Configuration constants
MAX_FILE_SIZE = 100 * 1024  # 100KB
MAX_LINES_PER_FILE = 500
BINARY_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.pdf', '.exe', '.dll', '.so', '.dylib', '.zip', '.tar', '.gz', '.mp4', '.mp3', '.wav', '.avi', '.mov', '.bin', '.pyc', '.class', '.ico', '.ttf', '.woff', '.woff2', '.eot'}
AUTO_GENERATED_PATTERNS = [
    r'.*\.min\.(js|css)$',  # Minified JS/CSS
    r'.*\.bundle\.js$',     # Bundled JS
    r'.*\.map$',            # Source maps
    r'.*\.lock$',           # Lock files (package-lock, etc)
    r'.*\.generated\.',     # Generated files
    r'.*\.auto\.',          # Auto-generated files
    r'.*\.d\.ts$',          # TypeScript declaration files
    r'.*-lock\.json$',      # npm/yarn lock files
    r'.*\.(log|logs)$',     # Log files
]
TRIVIAL_FILES = {
    '.editorconfig', '.eslintignore', '.prettierignore', 
    'thumbs.db', '.DS_Store', 'desktop.ini', 'LICENSE', 'LICENSE.md', 
    'LICENSE.txt', 'CONTRIBUTING.md', 'CODE_OF_CONDUCT.md'
}
# Files that should have their structure summarized, not full content
SUMMARIZE_FILES = {
    'package-lock.json', 'yarn.lock', 'poetry.lock', 'Pipfile.lock',
    'composer.lock', 'Gemfile.lock', 'Cargo.lock'
}
# Important files that should be included even if large
IMPORTANT_FILES = {
    'README.md', 'README.rst', 'README.txt', 'setup.py', 'setup.cfg',
    'package.json', 'requirements.txt', 'Gemfile', 'Cargo.toml',
    'Pipfile', 'pyproject.toml', 'composer.json', '.gitignore',
    'Dockerfile', 'docker-compose.yml', '.env.example'
}

def parse_exclusion_file(file_path: str) -> Set[str]:
    patterns = set()
    if file_path and os.path.exists(file_path):
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    patterns.add(line)
    return patterns

def is_excluded(path: str, exclusion_patterns: Set[str]) -> bool:
    for pattern in exclusion_patterns:
        if pattern.startswith('/') and pattern.endswith('/'):
            if path.startswith(pattern[1:]) or path == pattern[1:-1]:
                return True
        elif pattern.endswith('/'):
            if path.startswith(pattern) or path == pattern[:-1]:
                return True
        elif pattern.startswith('/'):
            if path == pattern[1:] or path.startswith(pattern[1:] + os.sep):
                return True
        else:
            if fnmatch.fnmatch(path, pattern) or any(fnmatch.fnmatch(part, pattern) for part in path.split(os.sep)):
                return True
    return False

def is_binary_file(file_path: str) -> bool:
    """Check if a file is binary based on extension and content."""
    # Check extension
    _, ext = os.path.splitext(file_path.lower())
    if ext in BINARY_EXTENSIONS:
        return True
    
    # Check MIME type
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type and not mime_type.startswith('text/'):
        return True
    
    # Check content for binary characters
    try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            if b'\0' in chunk:  # Null byte is a good indicator of binary file
                return True
            # Check for high proportion of non-printable characters
            text_chars = bytearray({7,8,9,10,12,13,27} | set(range(0x20, 0x100)) - {0x7f})
            non_text = len([b for b in chunk if b not in text_chars])
            if non_text / len(chunk) > 0.3:
                return True
    except:
        pass
    return False

def is_auto_generated(file_path: str) -> bool:
    """Check if a file is auto-generated."""
    for pattern in AUTO_GENERATED_PATTERNS:
        if re.match(pattern, file_path):
            return True
    
    # Check first few lines for generation markers
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            first_lines = [f.readline() for _ in range(5)]
            for line in first_lines:
                if any(marker in line.lower() for marker in ['auto-generated', 'automatically generated', 'do not edit', 'generated by']):
                    return True
    except:
        pass
    return False

def is_trivial_file(file_name: str) -> bool:
    """Check if a file is trivial and can be skipped."""
    return file_name.lower() in TRIVIAL_FILES

def truncate_content(content: str, file_rel_path: str, is_important: bool = False) -> str:
    """Intelligently truncate file content if too large."""
    lines = content.splitlines()
    file_name = os.path.basename(file_rel_path)
    
    # Don't truncate important files unless they're really large
    if is_important and len(content) <= MAX_FILE_SIZE * 2:
        return content
    
    if len(lines) <= MAX_LINES_PER_FILE and len(content) <= MAX_FILE_SIZE:
        return content
    
    # For code files, try to preserve structure
    if file_rel_path.endswith(('.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.go', '.rs', '.rb', '.php')):
        truncated_lines = []
        
        # Keep first 50 lines (usually imports, class definitions)
        truncated_lines.extend(lines[:50])
        truncated_lines.append("\n... [TRUNCATED - Showing first 50 and last 50 lines] ...\n")
        
        # Keep last 50 lines (often important methods/exports)
        truncated_lines.extend(lines[-50:])
        
        return '\n'.join(truncated_lines)
    else:
        # For other files, just truncate
        truncated_lines = lines[:MAX_LINES_PER_FILE]
        truncated_lines.append(f"\n... [TRUNCATED - File has {len(lines)} lines, showing first {MAX_LINES_PER_FILE}] ...\n")
        return '\n'.join(truncated_lines)

def get_file_info(file_path: str) -> Dict[str, any]:
    """Get file information including size and type."""
    try:
        stat = os.stat(file_path)
        file_name = os.path.basename(file_path)
        return {
            'size': stat.st_size,
            'size_str': format_file_size(stat.st_size),
            'binary': is_binary_file(file_path),
            'auto_generated': is_auto_generated(file_path),
            'trivial': is_trivial_file(file_name),
            'should_summarize': file_name in SUMMARIZE_FILES,
            'is_important': file_name in IMPORTANT_FILES
        }
    except:
        return {'size': 0, 'size_str': '0B', 'binary': False, 'auto_generated': False, 'trivial': False, 'should_summarize': False, 'is_important': False}

def format_file_size(size: int) -> str:
    """Format file size in human-readable format."""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size < 1024.0:
            return f"{size:.1f}{unit}"
        size /= 1024.0
    return f"{size:.1f}TB"

def summarize_lock_file(content: str, file_name: str) -> str:
    """Create a summary of lock files instead of full content."""
    lines = content.splitlines()
    summary = [f"[LOCK FILE SUMMARY: {file_name}]"]
    
    # Count dependencies
    dependency_count = 0
    
    if file_name == 'package-lock.json':
        # Count unique packages in npm lock file
        for line in lines:
            if '"version":' in line and '"node_modules/' in line:
                dependency_count += 1
    elif file_name in ['yarn.lock', 'Gemfile.lock']:
        # Count packages in yarn/bundler lock files
        for line in lines:
            if line and not line.startswith(' ') and not line.startswith('#'):
                dependency_count += 1
    elif file_name == 'Pipfile.lock':
        # Count packages in Pipenv lock file
        for line in lines:
            if '"version":' in line:
                dependency_count += 1
    else:
        # Generic counting for other lock files
        dependency_count = len([l for l in lines if l.strip() and not l.startswith('#')])
    
    summary.append(f"Total dependencies: ~{dependency_count}")
    summary.append(f"File size: {format_file_size(len(content))}")
    summary.append(f"[Full content omitted - this is a lock file with dependency versions]")
    
    return '\n'.join(summary)

def print_directory_structure(start_path: str, exclusion_patterns: Set[str]) -> str:
    def _generate_tree(dir_path: str, prefix: str = '') -> List[str]:
        entries = os.listdir(dir_path)
        entries = sorted(entries, key=lambda x: (not os.path.isdir(os.path.join(dir_path, x)), x.lower()))
        tree = []
        for i, entry in enumerate(entries):
            rel_path = os.path.relpath(os.path.join(dir_path, entry), start_path)
            if is_excluded(rel_path, exclusion_patterns):
                continue
            
            if i == len(entries) - 1:
                connector = '└── '
                new_prefix = prefix + '    '
            else:
                connector = '├── '
                new_prefix = prefix + '│   '
            
            full_path = os.path.join(dir_path, entry)
            if os.path.isdir(full_path):
                tree.append(f"{prefix}{connector}{entry}/")
                tree.extend(_generate_tree(full_path, new_prefix))
            else:
                # Add file size and type info
                info = get_file_info(full_path)
                size_str = f" ({info['size_str']})" if info['size'] > 0 else ""
                binary_str = " [binary]" if info['binary'] else ""
                tree.append(f"{prefix}{connector}{entry}{size_str}{binary_str}")
        return tree

    tree = ['/ '] + _generate_tree(start_path)
    return '\n'.join(tree)

def analyze_codebase(start_path: str, exclusion_patterns: Set[str]) -> Dict[str, any]:
    """Analyze the codebase to provide AI-relevant summary."""
    analysis = {
        'primary_language': None,
        'frameworks': [],
        'project_type': None,
        'config_files': [],
        'test_directories': [],
        'build_files': [],
        'docs_directories': [],
        'total_files': 0,
        'file_types': {}
    }
    
    # Common framework/config file mappings
    framework_markers = {
        'package.json': 'Node.js',
        'requirements.txt': 'Python',
        'setup.py': 'Python',
        'pyproject.toml': 'Python',
        'Gemfile': 'Ruby',
        'Cargo.toml': 'Rust',
        'go.mod': 'Go',
        'composer.json': 'PHP',
        'pom.xml': 'Java/Maven',
        'build.gradle': 'Java/Gradle',
        '.csproj': '.NET',
        'tsconfig.json': 'TypeScript',
        'angular.json': 'Angular',
        'vue.config.js': 'Vue.js',
        'next.config.js': 'Next.js',
        'gatsby-config.js': 'Gatsby',
        'webpack.config.js': 'webpack',
        'Makefile': 'Make',
        'CMakeLists.txt': 'CMake',
        'Dockerfile': 'Docker',
        'docker-compose.yml': 'Docker Compose',
        '.github/workflows': 'GitHub Actions',
        '.gitlab-ci.yml': 'GitLab CI'
    }
    
    for root, dirs, files in os.walk(start_path):
        rel_path = os.path.relpath(root, start_path)
        
        if is_excluded(rel_path, exclusion_patterns):
            continue
            
        # Check for test directories
        for dir_name in dirs:
            lower_dir = dir_name.lower()
            if any(test_name in lower_dir for test_name in ['test', 'tests', 'spec', 'specs']):
                analysis['test_directories'].append(os.path.join(rel_path, dir_name))
            if any(doc_name in lower_dir for doc_name in ['docs', 'documentation']):
                analysis['docs_directories'].append(os.path.join(rel_path, dir_name))
        
        for file in files:
            file_rel_path = os.path.join(rel_path, file)
            if is_excluded(file_rel_path, exclusion_patterns):
                continue
                
            analysis['total_files'] += 1
            
            # Track file extensions
            _, ext = os.path.splitext(file)
            if ext:
                analysis['file_types'][ext] = analysis['file_types'].get(ext, 0) + 1
            
            # Check for framework markers
            for marker, framework in framework_markers.items():
                if file == marker or file.endswith(marker):
                    if framework not in analysis['frameworks']:
                        analysis['frameworks'].append(framework)
                    analysis['config_files'].append(file_rel_path)
            
            # Check for build files
            if file in ['Makefile', 'CMakeLists.txt', 'build.gradle', 'pom.xml', 'setup.py']:
                analysis['build_files'].append(file_rel_path)
    
    # Determine primary language based on file extensions
    if analysis['file_types']:
        lang_extensions = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.ts': 'TypeScript',
            '.java': 'Java',
            '.cs': 'C#',
            '.cpp': 'C++',
            '.c': 'C',
            '.go': 'Go',
            '.rs': 'Rust',
            '.rb': 'Ruby',
            '.php': 'PHP',
            '.swift': 'Swift',
            '.kt': 'Kotlin',
        }
        
        # Find the most common programming language extension
        max_count = 0
        for ext, lang in lang_extensions.items():
            count = analysis['file_types'].get(ext, 0)
            if count > max_count:
                max_count = count
                analysis['primary_language'] = lang
    
    # Determine project type
    if 'React' in analysis['frameworks'] or 'Vue.js' in analysis['frameworks'] or 'Angular' in analysis['frameworks']:
        analysis['project_type'] = 'Frontend Web Application'
    elif 'Django' in analysis['frameworks'] or 'Flask' in analysis['frameworks']:
        analysis['project_type'] = 'Python Web Application'
    elif 'Express' in analysis['frameworks'] or 'Next.js' in analysis['frameworks']:
        analysis['project_type'] = 'Node.js Web Application'
    elif analysis['test_directories']:
        analysis['project_type'] = 'Library/Package with Tests'
    else:
        analysis['project_type'] = 'General Software Project'
    
    return analysis

def scan_folder(start_path: str, file_types: Optional[List[str]], output_file: str, exclusion_patterns: Set[str]) -> None:
    with open(output_file, 'w', encoding='utf-8') as out_file:
        # Statistics
        stats = {
            'total_files': 0,
            'processed_files': 0,
            'binary_files': 0,
            'auto_generated_files': 0,
            'truncated_files': 0,
            'skipped_files': 0,
            'total_size': 0,
            'processed_size': 0
        }
        
        # Analyze the codebase first
        codebase_analysis = analyze_codebase(start_path, exclusion_patterns)
        
        # Write AI-focused summary
        out_file.write("AI-Optimized Codebase Summary:\n")
        out_file.write("=" * 30 + "\n")
        out_file.write(f"Project Type: {codebase_analysis['project_type']}\n")
        out_file.write(f"Primary Language: {codebase_analysis['primary_language'] or 'Not determined'}\n")
        if codebase_analysis['frameworks']:
            out_file.write(f"Frameworks/Tools: {', '.join(codebase_analysis['frameworks'])}\n")
        out_file.write(f"Total Files: {codebase_analysis['total_files']}\n")
        
        if codebase_analysis['test_directories']:
            out_file.write(f"Test Directories: {', '.join(codebase_analysis['test_directories'][:3])}{'...' if len(codebase_analysis['test_directories']) > 3 else ''}\n")
        if codebase_analysis['build_files']:
            out_file.write(f"Build Files: {', '.join(codebase_analysis['build_files'][:3])}{'...' if len(codebase_analysis['build_files']) > 3 else ''}\n")
        
        out_file.write("\nKey Insights for AI/LLM Processing:\n")
        out_file.write("- Files are truncated to show structure (first/last 50 lines for code)\n")
        out_file.write("- Binary files and auto-generated content are excluded\n")
        out_file.write("- Lock files are summarized instead of showing full content\n")
        out_file.write("- Important config files are preserved even if large\n")
        out_file.write("\n")
        
        # Write the directory structure
        out_file.write("Directory Structure:\n")
        out_file.write("-------------------\n")
        out_file.write(print_directory_structure(start_path, exclusion_patterns))
        out_file.write("\n\n")
        out_file.write("File Contents:\n")
        out_file.write("--------------\n")

        for root, dirs, files in os.walk(start_path):
            rel_path = os.path.relpath(root, start_path)
            
            if is_excluded(rel_path, exclusion_patterns):
                continue
            
            for file in files:
                file_rel_path = os.path.join(rel_path, file)
                if is_excluded(file_rel_path, exclusion_patterns):
                    continue
                
                if file_types is None or any(file.endswith(ext) for ext in file_types):
                    file_path = os.path.join(root, file)
                    file_info = get_file_info(file_path)
                    
                    stats['total_files'] += 1
                    stats['total_size'] += file_info['size']
                    
                    # Skip binary files
                    if file_info['binary']:
                        stats['binary_files'] += 1
                        stats['skipped_files'] += 1
                        print(f"Skipping binary file: {file_rel_path}")
                        continue
                    
                    # Skip auto-generated files unless they're important
                    if file_info['auto_generated'] and not file_info['is_important']:
                        stats['auto_generated_files'] += 1
                        stats['skipped_files'] += 1
                        print(f"Skipping auto-generated file: {file_rel_path}")
                        continue
                    
                    # Skip trivial files unless they're important
                    if file_info['trivial'] and not file_info['is_important']:
                        stats['skipped_files'] += 1
                        print(f"Skipping trivial file: {file_rel_path}")
                        continue
                    
                    print(f"Processing: {file_rel_path}")
                    out_file.write(f"File: {file_rel_path} ({file_info['size_str']})\n")
                    out_file.write("-" * 50 + "\n")
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as in_file:
                            content = in_file.read()
                            
                            # Handle lock files specially
                            if file_info['should_summarize']:
                                content = summarize_lock_file(content, file)
                            else:
                                # Check if truncation is needed
                                original_length = len(content)
                                content = truncate_content(content, file_rel_path, file_info['is_important'])
                                if len(content) < original_length:
                                    stats['truncated_files'] += 1
                            
                            out_file.write(f"Content of {file_rel_path}:\n")
                            out_file.write(content)
                            stats['processed_files'] += 1
                            stats['processed_size'] += file_info['size']
                    except Exception as e:
                        print(f"Error reading file {file_rel_path}: {str(e)}. Skipping.")
                        out_file.write(f"Error reading file: {str(e)}. Content skipped.\n")
                        stats['skipped_files'] += 1
                    
                    out_file.write("\n\n")
        
        # Write statistics
        out_file.write("\n" + "="*50 + "\n")
        out_file.write("Processing Summary:\n")
        out_file.write("-------------------\n")
        out_file.write(f"Total files found: {stats['total_files']}\n")
        out_file.write(f"Files processed: {stats['processed_files']}\n")
        out_file.write(f"Binary files skipped: {stats['binary_files']}\n")
        out_file.write(f"Auto-generated files skipped: {stats['auto_generated_files']}\n")
        out_file.write(f"Files truncated: {stats['truncated_files']}\n")
        out_file.write(f"Total files skipped: {stats['skipped_files']}\n")
        out_file.write(f"Total size: {format_file_size(stats['total_size'])}\n")
        out_file.write(f"Processed size: {format_file_size(stats['processed_size'])}\n")

def main(args: List[str]) -> None:
    if len(args) < 3:
        print("Usage: python script.py <start_path> <output_file> [exclusion_file] [file_extensions...]")
        print("Both exclusion_file and file_extensions are optional.")
        sys.exit(1)

    start_path: str = args[1]
    output_file: str = args[2]
    exclusion_file: Optional[str] = None
    file_types: Optional[List[str]] = None

    if len(args) > 3:
        if not args[3].startswith('.'):
            exclusion_file = args[3]
            file_types = args[4:] if len(args) > 4 else None
        else:
            file_types = args[3:]

    exclusion_patterns = parse_exclusion_file(exclusion_file) if exclusion_file else set()
    
    if exclusion_file:
        print(f"Using exclusion patterns from {exclusion_file}: {exclusion_patterns}")
    else:
        print("No exclusion file specified. Scanning all files.")

    if file_types:
        print(f"Scanning for file types: {file_types}")
    else:
        print("No file types specified. Scanning all files.")

    scan_folder(start_path, file_types, output_file, exclusion_patterns)
    print(f"Scan complete. Results written to {output_file}")

if __name__ == "__main__":
    main(sys.argv)